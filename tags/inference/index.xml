<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Inference on Dev. note</title><link>http://huijeong-kim.github.io/tags/inference/</link><description>Recent content in Inference on Dev. note</description><generator>Hugo -- 0.146.0</generator><language>kr</language><lastBuildDate>Sat, 10 May 2025 20:46:32 +0900</lastBuildDate><atom:link href="http://huijeong-kim.github.io/tags/inference/index.xml" rel="self" type="application/rss+xml"/><item><title>Transformer Architecture - 구조와 연산 소개</title><link>http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/</link><pubDate>Sat, 10 May 2025 20:46:32 +0900</pubDate><guid>http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/</guid><description>&lt;p>&lt;em>‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 1편&lt;/em>&lt;/p>
&lt;p>Transformer Inference의 성능 병목을 이해하기 위해 ‘Full Stack Optimization of Transformer Inference: a Survey’ 논문을 읽고 있습니다. 이번 글은 본격적인 분석에 앞서 Transformer의 기본 구조와 이를 구성하는 주요 연산들을 간략히 소개합니다.&lt;/p>
&lt;p>논문 링크: &lt;a href="https://arxiv.org/abs/2302.14017">https://arxiv.org/abs/2302.14017&lt;/a>&lt;/p>
&lt;p> 
 &lt;/p>
&lt;hr>
&lt;h2 id="transformer-architecture">Transformer Architecture&lt;/h2>
&lt;p>Transformer는 입력을 인코딩하는 &lt;strong>Encoder&lt;/strong>와 출력을 생성하는 &lt;strong>Decoder&lt;/strong>로 구성됩니다. 두 모듈 모두 &lt;strong>Multi-Head Attention (MHA)&lt;/strong> 과 &lt;strong>Feed-Forward Network (FFN)&lt;/strong> 으로 이루어진 &lt;strong>Transformer Block&lt;/strong>을 반복적으로 쌓아 구현됩니다.&lt;/p></description></item></channel></rss>