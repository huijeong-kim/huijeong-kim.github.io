<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Machine Learning on Dev. note</title><link>http://huijeong-kim.github.io/tags/machine-learning/</link><description>Recent content in Machine Learning on Dev. note</description><generator>Hugo -- 0.146.0</generator><language>kr</language><lastBuildDate>Mon, 26 May 2025 07:31:04 +0900</lastBuildDate><atom:link href="http://huijeong-kim.github.io/tags/machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Transformer Model Workload Analysis</title><link>http://huijeong-kim.github.io/post/2025-05-26-transformer-model-workload-analysis/</link><pubDate>Mon, 26 May 2025 07:31:04 +0900</pubDate><guid>http://huijeong-kim.github.io/post/2025-05-26-transformer-model-workload-analysis/</guid><description>&lt;p>&lt;em>‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 2편&lt;/em>&lt;/p>
&lt;p>논문의 2.2장에서는 모델 워크로드를 분석합니다. Idle한 상황을 가정하고, 각 트랜스포머 모델의 이론적 최대 성능(upper bound)을 분석합니다. 그 과정에서 각 모델의 특성을 이해해 볼 수 있습니다.&lt;/p>
&lt;p>논문 링크: &lt;a href="https://arxiv.org/abs/2302.14017">https://arxiv.org/abs/2302.14017&lt;/a>&lt;/p>
&lt;hr>
&lt;p> &lt;/p>
&lt;h2 id="models">Models&lt;/h2>
&lt;p>논문은 BERT-Base, BERT-Large, GPT-2 모델로 워크로드를 분석했습니다. 모두 트랜스포머 기반입니다. 주요 특징과 parameter configuration은 아래와 같습니다.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>모델&lt;/th>
&lt;th>구조&lt;/th>
&lt;th>방향성&lt;/th>
&lt;th>목적&lt;/th>
&lt;th>주요 용도&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>12-layer BERT-Base&lt;/td>
&lt;td>인코더-only&lt;/td>
&lt;td>양방향&lt;/td>
&lt;td>마스킹된 단어 예측 (MLM)&lt;/td>
&lt;td>문장 이해 (분류, 질의응답 등)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>24-layer BERT-Large&lt;/td>
&lt;td>인코더-only&lt;/td>
&lt;td>양방향&lt;/td>
&lt;td>BERT-Base 확장&lt;/td>
&lt;td>고성능 문장 이해&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12-layer GPT-2&lt;/td>
&lt;td>디코더-only&lt;/td>
&lt;td>단방향 (왼→오)&lt;/td>
&lt;td>다음 단어 예측 (causal LM)&lt;/td>
&lt;td>텍스트 생성 (요약, 번역, 대화 등)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p> &lt;/p></description></item><item><title>Transformer Architecture - 구조와 연산 소개</title><link>http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/</link><pubDate>Sat, 10 May 2025 20:46:32 +0900</pubDate><guid>http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/</guid><description>&lt;p>&lt;em>‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 1편&lt;/em>&lt;/p>
&lt;p>Transformer Inference의 성능 병목을 이해하기 위해 ‘Full Stack Optimization of Transformer Inference: a Survey’ 논문을 읽고 있습니다. 이번 글은 본격적인 분석에 앞서 Transformer의 기본 구조와 이를 구성하는 주요 연산들을 간략히 소개합니다.&lt;/p>
&lt;p>논문 링크: &lt;a href="https://arxiv.org/abs/2302.14017">https://arxiv.org/abs/2302.14017&lt;/a>&lt;/p>
&lt;p> 
 &lt;/p>
&lt;hr>
&lt;h2 id="transformer-architecture">Transformer Architecture&lt;/h2>
&lt;p>Transformer는 입력을 인코딩하는 &lt;strong>Encoder&lt;/strong>와 출력을 생성하는 &lt;strong>Decoder&lt;/strong>로 구성됩니다. 두 모듈 모두 &lt;strong>Multi-Head Attention (MHA)&lt;/strong> 과 &lt;strong>Feed-Forward Network (FFN)&lt;/strong> 으로 이루어진 &lt;strong>Transformer Block&lt;/strong>을 반복적으로 쌓아 구현됩니다.&lt;/p></description></item><item><title>ML Strategy 2</title><link>http://huijeong-kim.github.io/post/2023-11-18-ml-strategy-2/</link><pubDate>Fri, 17 Nov 2023 19:20:54 +0900</pubDate><guid>http://huijeong-kim.github.io/post/2023-11-18-ml-strategy-2/</guid><description>&lt;p>지난번에 이어, Andrew Ng 교수님의 &amp;ldquo;ML Strategy&amp;rdquo; 강의 이어서 정리합니다. 이번에는 ML 시스템 디버깅과 프로젝트 디자인 팁들이 소개됩니다.&lt;/p>
&lt;p> &lt;/p>
&lt;hr>
&lt;h1 id="error-analysis">Error Analysis&lt;/h1>
&lt;p>사람이 할 수 있는 일에 대한 learning algorithm이 아직 human performance에 미치지 못한다면, machine의 mistake를 수동으로(manually) 분석하여 인사이트를 얻고 추가 개선을 해 볼 수 있습니다. 이 과정을 &lt;code>Error Analysis&lt;/code>라고 합니다.&lt;/p>
&lt;p> &lt;/p>
&lt;h2 id="carrying-out-error-analysis">Carrying Out Error Analysis&lt;/h2>
&lt;p>Error analysis를 통해 시스템의 &amp;ldquo;어떤&amp;rdquo; 성능을 높이는 게 좋을지 찾아볼 수 있습니다. Cat classifier의 예를 들면, 전체적인 성능을 높이는 것이 아니라 &amp;ldquo;dog picture가 cat으로 분류되는 경우 줄이기(false positive 줄이기)&amp;ldquo;와 같은 구체적인 목표를 세울 수 있습니다.&lt;/p></description></item><item><title>ML Strategy 1</title><link>http://huijeong-kim.github.io/post/2023-11-09-ml-strategy-1/</link><pubDate>Thu, 09 Nov 2023 14:56:31 +0900</pubDate><guid>http://huijeong-kim.github.io/post/2023-11-09-ml-strategy-1/</guid><description>&lt;p>최근 Andrew Ng 교수님의 Deep Learning Specialization(DLS) 과정을 들었습니다. Machine Learning이라는 완전히 다른 패러다임에 적응하지 어렵고 오랫만에 보는 수식들에 정신이 없지만, 그래도 나름 재미있게 공부하고 있어요.&lt;/p>
&lt;p>DLS 강의 중 &amp;ldquo;ML Strategy&amp;rdquo; 강의가 특히 흥미로웠습니다. 이 강의를 통해 &amp;ldquo;ML engineering&amp;quot;을 어떻게 하는지 엿볼 수 있었기 때문입니다. 처음에는 ML System의 general 성능(실행속도 아닌 정확도를 의미)을 측정하는 게 불가능해 보였지만, 이 강의를 통해 ML System을 체계적으로, 전략적으로 분석, 개선하는 방법을 (조금이나마) 배울 수 있었습니다. 개인적으로 기억에 남는 강의여서 이 부분만 따로 정리해 올려봅니다. 총 2편으로 나눠집니다.&lt;/p></description></item></channel></rss>