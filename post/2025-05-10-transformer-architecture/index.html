<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformer Architecture - 구조와 연산 소개 | Dev. note</title>
<meta name=keywords content="machine learning,inference"><meta name=description content="‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 1편
Transformer Inference의 성능 병목을 이해하기 위해 ‘Full Stack Optimization of Transformer Inference: a Survey’ 논문을 읽고 있습니다. 이번 글은 본격적인 분석에 앞서 Transformer의 기본 구조와 이를 구성하는 주요 연산들을 간략히 소개합니다.
논문 링크: https://arxiv.org/abs/2302.14017
 
 

Transformer Architecture
Transformer는 입력을 인코딩하는 Encoder와 출력을 생성하는 Decoder로 구성됩니다. 두 모듈 모두 Multi-Head Attention (MHA) 과 Feed-Forward Network (FFN) 으로 이루어진 Transformer Block을 반복적으로 쌓아 구현됩니다."><meta name=author content="Huijeong Kim"><link rel=canonical href=http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/><link crossorigin=anonymous href=../../assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://huijeong-kim.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://huijeong-kim.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://huijeong-kim.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://huijeong-kim.github.io/apple-touch-icon.png><link rel=mask-icon href=http://huijeong-kim.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-VHBQNPGKKB"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-VHBQNPGKKB")}</script><meta property="og:url" content="http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/"><meta property="og:site_name" content="Dev. note"><meta property="og:title" content="Transformer Architecture - 구조와 연산 소개"><meta property="og:description" content="‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 1편
Transformer Inference의 성능 병목을 이해하기 위해 ‘Full Stack Optimization of Transformer Inference: a Survey’ 논문을 읽고 있습니다. 이번 글은 본격적인 분석에 앞서 Transformer의 기본 구조와 이를 구성하는 주요 연산들을 간략히 소개합니다.
논문 링크: https://arxiv.org/abs/2302.14017
Transformer Architecture Transformer는 입력을 인코딩하는 Encoder와 출력을 생성하는 Decoder로 구성됩니다. 두 모듈 모두 Multi-Head Attention (MHA) 과 Feed-Forward Network (FFN) 으로 이루어진 Transformer Block을 반복적으로 쌓아 구현됩니다."><meta property="og:locale" content="kr"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2025-05-10T20:46:32+09:00"><meta property="article:modified_time" content="2025-05-10T20:46:32+09:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Inference"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformer Architecture - 구조와 연산 소개"><meta name=twitter:description content="‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 1편
Transformer Inference의 성능 병목을 이해하기 위해 ‘Full Stack Optimization of Transformer Inference: a Survey’ 논문을 읽고 있습니다. 이번 글은 본격적인 분석에 앞서 Transformer의 기본 구조와 이를 구성하는 주요 연산들을 간략히 소개합니다.
논문 링크: https://arxiv.org/abs/2302.14017
 
 

Transformer Architecture
Transformer는 입력을 인코딩하는 Encoder와 출력을 생성하는 Decoder로 구성됩니다. 두 모듈 모두 Multi-Head Attention (MHA) 과 Feed-Forward Network (FFN) 으로 이루어진 Transformer Block을 반복적으로 쌓아 구현됩니다."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://huijeong-kim.github.io/post/"},{"@type":"ListItem","position":2,"name":"Transformer Architecture - 구조와 연산 소개","item":"http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer Architecture - 구조와 연산 소개","name":"Transformer Architecture - 구조와 연산 소개","description":"‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 1편\nTransformer Inference의 성능 병목을 이해하기 위해 ‘Full Stack Optimization of Transformer Inference: a Survey’ 논문을 읽고 있습니다. 이번 글은 본격적인 분석에 앞서 Transformer의 기본 구조와 이를 구성하는 주요 연산들을 간략히 소개합니다.\n논문 링크: https://arxiv.org/abs/2302.14017\nTransformer Architecture Transformer는 입력을 인코딩하는 Encoder와 출력을 생성하는 Decoder로 구성됩니다. 두 모듈 모두 Multi-Head Attention (MHA) 과 Feed-Forward Network (FFN) 으로 이루어진 Transformer Block을 반복적으로 쌓아 구현됩니다.\n","keywords":["machine learning","inference"],"articleBody":"‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 1편\nTransformer Inference의 성능 병목을 이해하기 위해 ‘Full Stack Optimization of Transformer Inference: a Survey’ 논문을 읽고 있습니다. 이번 글은 본격적인 분석에 앞서 Transformer의 기본 구조와 이를 구성하는 주요 연산들을 간략히 소개합니다.\n논문 링크: https://arxiv.org/abs/2302.14017\nTransformer Architecture Transformer는 입력을 인코딩하는 Encoder와 출력을 생성하는 Decoder로 구성됩니다. 두 모듈 모두 Multi-Head Attention (MHA) 과 Feed-Forward Network (FFN) 으로 이루어진 Transformer Block을 반복적으로 쌓아 구현됩니다.\nTransformer Blocks - Encoder Transformer는 여러 개의 Transformer Block으로 구성되며, 각 Block은 다음 두 가지 모듈로 구성됩니다:\nMulti-Head Attention (MHA) Feed-Forward Network (FFN) 각 모듈에는 Layer Normalization (LayerNorm) 과 Residual Connection이 함께 적용되어 안정적인 학습과 추론을 도와줍니다.\n입력 시퀀스는 길이 $l$ 의 토큰으로 구성되며, 각 토큰은 차원 $d$ 의 벡터로 표현됩니다.\nMulti-Head Attention (MHA) MHA는 입력을 $h$개의 head로 나누어 병렬로 attention을 수행한 뒤, 결과를 병합합니다. 주요 단계는 다음과 같습니다:\nProjection: Query와 key 의 곱을 통해 attention score를 계산하고 softmax를 적용해 정규화 Weighted sum: 정규화 된 attention score와 Value를 곱하여 각 head의 출력을 얻음 Merge and Projection: 모든 head의 출력을 병합한 뒤 $W_{out}$ 과 곱하여 출력 생성 Normalization and Residual Connection: LayerNorm 적용하고 입력을 더하여 최종 출력 생성 MHA에는 총 6개의 matrix multiplication이 포함됩니다. 이 중:\n4개는 weight × activation 연산: $W_Q$, $W_K$, $W_V$, $W_{out}$ (초록색) 2개는 activation × activation 연산: $Q \\cdot K^\\top$, attention × $V$ (빨간색) 이 둘은 연산 방식은 같지만 데이터 재사용 및 성능 특성은 다릅니다. 자세한 내용은 다음 글에서 다룰 예정입니다.\nFeed-Forward Network (FFN) FFN은 다음과 같은 구조를 가집니다:\nLinear: 입력을 차원 $d \\rightarrow d_{FFN}$으로 확장 GELU Activation Linear: 차원을 다시 $d_{FFN} \\rightarrow d$로 축소 Residual + LayerNorm FFN에는 2개의 linear 연산이 포함됩니다.\nLinear 연산: Matmul Dimension 정리 아래 표는 각 모듈의 linear 연산과 연산 차원을 요약한 것입니다. Q·K^T와 attention·V는 **시퀀스 길이에 대해 quadratic하고, 나머지는 linear합니다.\nModule Operation Matmul dim MHA ${W_Q}$ projection $d$ x $d$ x $l$ ${W_K}$ projection $d$ x $d$ x $l$ ${W_V}$ projection $d$ x $d$ x $l$ query x key $l$ x $d/h$ x $l$ atten. score x value $d/h$ x $l$ x $l$ ${W_{out}}$ projection $d$ x $d$ x $l$ FFN ${W_1}$ projection ${d_{FFN}}$ x $d$ x $l$ ${W_2}$ projection ${d}$ x ${d_{FFN}}$ x ${l}$ $l$ : sequence length $d$ : hidden dimension $h$ : attention heads Non-linear 연산 Transformer에는 다음과 같은 비선형 연산이 포함됩니다: Softmax, LayerNorm, GELU\n이들은 연산량은 적지만 메모리 접근 패턴과 연산 방식 때문에 오버헤드가 발생하기 쉽습니다. 논문의 다음 그림은 각 non-linear 연산이 어느 축으로 작동하고 어떻게 동작하는지 잘 보여줍니다. 초록색 표시가 계산 방향을 나타냅니다. (a) Softmax - 시퀀스 길이를 따라 정규화 Softmax는 주로 어텐션 점수를 정규화할 때 사용되며, 한 행(= 시퀀스 길이) 안에서 exponential 값을 모두 더한 후 각 항목을 나눕니다.\n계산 방향: Sequence Length 축 특징: 전체 합을 먼저 계산해야 softmax를 적용할 수 있음 한 번 전체를 스캔해야 하므로 1-pass로는 부족 성능 이슈: 입력 전체를 순회하며 합을 구해야 하므로 메모리 접근 비용이 큼 (b) Layer Normalization – 히든 차원으로 정규화 LayerNorm은 Transformer에서 거의 모든 블록 뒤에 붙는 핵심 연산입니다. 각 토큰의 히든 벡터를 기준으로 평균과 표준편차를 계산하고 정규화합니다.\n계산 방향: Hidden Dimension 축 (한 토큰 기준) 특징: 평균(μ), 표준편차(σ) 계산 → 정규화 → scale \u0026 shift 따라서 최소 3번 이상 입력을 순회해야 함 성능 이슈: 반복된 접근이 필요해서 메모리 대역폭 병목의 주요 원인 중 하나 (c) Batch Normalization – 채널 단위, 학습된 통계 사용 BatchNorm은 주로 CNN에서 사용되고, 훈련 중에 채널별 통계(μ, σ)를 학습하여 저장합니다. 추론 시에는 학습된 평균과 표준편차를 그대로 사용하므로 속도가 빠릅니다.\n계산 방향: Channel 축 (Across batch/height/width) 특징: μ, σ 모두 훈련 중 학습되고 고정됨 추론 시 1-pass로 충분 (실시간 계산 없음) 성능 장점: 추론 시 가장 빠름 연산량 대비 메모리 접근도 적음 Transformer Block - Decoder Decoder는 Encoder와 유사하지만 다음과 같은 차이가 있습니다:\n입력은 한 토큰 ($d × 1$)만 처리 이전 토큰들과 concat하여 MHA 수행 (auto-regressive) 미래 토큰을 참조하지 않도록 Masked Attention 적용 Attention은 matrix-vector 연산으로 수행됨 [ $d/h$ x $1$, $l$ x $d/h$ ] Encoder vs. Decoder: 구조 비교 항목 인코더 (Encoder) 디코더 (Decoder) 입력 처리 방식 전체 입력 시퀀스를 병렬적으로 처리 이전에 생성된 토큰들을 사용하여 순차적으로 처리 (auto-regressive) 기본 구성 Multi-Head Self Attention (MHA)Feed-Forward Network (FFN) Masked Multi-Head Self AttentionCross Attention (인코더 출력과 함께)Feed-Forward Network (FFN) Self-Attention 입력 전체에 대해 자유롭게 어텐션 가능 미래 정보 차단을 위한 마스킹(masked) 어텐션 사용 Cross-Attention 존재 여부 없음 (입력 자체만 사용) 있음 (인코더의 출력과 디코더의 입력 간 어텐션) 입력/출력 예시 “I am a student” → 인코딩된 벡터 “나는 학생입니다” 생성 (한 토큰씩) 용도 자연어 이해 (e.g., 문장 분류, 요약) 자연어 생성 (e.g., 기계 번역, 텍스트 생성) 병렬성 높음 (모든 토큰을 동시에 처리 가능) 낮음 (각 토큰이 이전 토큰에 의존함) 캐시 활용 일반적으로 없음 Key/Value 캐싱으로 이전 연산 결과 재사용 → 성능 향상 Transformer의 대략적인 구조와 각 블락을 이루는 연산 단위를 살펴보았습니다. 다음 글에서는 논문의 2.2장 Model Analysis를 기반으로 BERT 모델과 GPT 모델의 병목 지점을 알아보겠습니다.\n","wordCount":"783","inLanguage":"en","datePublished":"2025-05-10T20:46:32+09:00","dateModified":"2025-05-10T20:46:32+09:00","author":{"@type":"Person","name":"Huijeong Kim"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/"},"publisher":{"@type":"Organization","name":"Dev. note","logo":{"@type":"ImageObject","url":"http://huijeong-kim.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://huijeong-kim.github.io/ accesskey=h title="Dev. note (Alt + H)">Dev. note</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://huijeong-kim.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://huijeong-kim.github.io/post/ title=Posts><span>Posts</span></a></li><li><a href=http://huijeong-kim.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://huijeong-kim.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://huijeong-kim.github.io/>Home</a>&nbsp;»&nbsp;<a href=http://huijeong-kim.github.io/post/>Posts</a></div><h1 class="post-title entry-hint-parent">Transformer Architecture - 구조와 연산 소개</h1><div class=post-meta><span title='2025-05-10 20:46:32 +0900 +0900'>2025-05-10</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;783 words&nbsp;·&nbsp;Huijeong Kim</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#transformer-architecture aria-label="Transformer Architecture">Transformer Architecture</a></li><li><a href=#transformer-blocks---encoder aria-label="Transformer Blocks - Encoder">Transformer Blocks - Encoder</a><ul><li><a href=#multi-head-attention-mha aria-label="Multi-Head Attention (MHA)">Multi-Head Attention (MHA)</a></li><li><a href=#feed-forward-network-ffn aria-label="Feed-Forward Network (FFN)">Feed-Forward Network (FFN)</a></li></ul></li><li><a href=#linear-%ec%97%b0%ec%82%b0-matmul-dimension-%ec%a0%95%eb%a6%ac aria-label="Linear 연산: Matmul Dimension 정리">Linear 연산: Matmul Dimension 정리</a></li><li><a href=#non-linear-%ec%97%b0%ec%82%b0 aria-label="Non-linear 연산">Non-linear 연산</a></li><li><a href=#transformer-block---decoder aria-label="Transformer Block - Decoder">Transformer Block - Decoder</a></li><li><a href=#encoder-vs-decoder-%ea%b5%ac%ec%a1%b0-%eb%b9%84%ea%b5%90 aria-label="Encoder vs. Decoder: 구조 비교">Encoder vs. Decoder: 구조 비교</a></li></ul></div></details></div><div class=post-content><p><em>‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 1편</em></p><p>Transformer Inference의 성능 병목을 이해하기 위해 ‘Full Stack Optimization of Transformer Inference: a Survey’ 논문을 읽고 있습니다. 이번 글은 본격적인 분석에 앞서 Transformer의 기본 구조와 이를 구성하는 주요 연산들을 간략히 소개합니다.</p><p>논문 링크: <a href=https://arxiv.org/abs/2302.14017>https://arxiv.org/abs/2302.14017</a></p><p> 
 </p><hr><h2 id=transformer-architecture>Transformer Architecture<a hidden class=anchor aria-hidden=true href=#transformer-architecture>#</a></h2><p>Transformer는 입력을 인코딩하는 <strong>Encoder</strong>와 출력을 생성하는 <strong>Decoder</strong>로 구성됩니다. 두 모듈 모두 <strong>Multi-Head Attention (MHA)</strong> 과 <strong>Feed-Forward Network (FFN)</strong> 으로 이루어진 <strong>Transformer Block</strong>을 반복적으로 쌓아 구현됩니다.</p><h2 id=transformer-blocks---encoder>Transformer Blocks - Encoder<a hidden class=anchor aria-hidden=true href=#transformer-blocks---encoder>#</a></h2><p>Transformer는 여러 개의 Transformer Block으로 구성되며, 각 Block은 다음 두 가지 모듈로 구성됩니다:</p><ul><li>Multi-Head Attention (MHA)</li><li>Feed-Forward Network (FFN)</li></ul><p>각 모듈에는 <strong>Layer Normalization (LayerNorm)</strong> 과 <strong>Residual Connection</strong>이 함께 적용되어 안정적인 학습과 추론을 도와줍니다.</p><p>입력 시퀀스는 길이 $l$ 의 토큰으로 구성되며, 각 토큰은 차원 $d$ 의 벡터로 표현됩니다.</p><p><img alt="transformer encoder" loading=lazy src=../../post/2025-05-10-transformer-architecture/image01.png></p><p> </p><h3 id=multi-head-attention-mha>Multi-Head Attention (MHA)<a hidden class=anchor aria-hidden=true href=#multi-head-attention-mha>#</a></h3><p>MHA는 입력을 $h$개의 head로 나누어 병렬로 attention을 수행한 뒤, 결과를 병합합니다. 주요 단계는 다음과 같습니다:</p><ol><li><strong>Projection</strong>: Query와 key 의 곱을 통해 attention score를 계산하고 softmax를 적용해 정규화</li><li><strong>Weighted sum</strong>: 정규화 된 attention score와 Value를 곱하여 각 head의 출력을 얻음</li><li><strong>Merge and Projection</strong>: 모든 head의 출력을 병합한 뒤 $W_{out}$ 과 곱하여 출력 생성</li><li><strong>Normalization and Residual Connection</strong>: LayerNorm 적용하고 입력을 더하여 최종 출력 생성</li></ol><p>MHA에는 총 6개의 matrix multiplication이 포함됩니다. 이 중:</p><ul><li><strong>4개</strong>는 weight × activation 연산: $W_Q$, $W_K$, $W_V$, $W_{out}$ (초록색)</li><li><strong>2개</strong>는 activation × activation 연산: $Q \cdot K^\top$, attention × $V$ (빨간색)</li></ul><p>이 둘은 연산 방식은 같지만 데이터 재사용 및 성능 특성은 다릅니다. 자세한 내용은 다음 글에서 다룰 예정입니다.</p><h3 id=feed-forward-network-ffn>Feed-Forward Network (FFN)<a hidden class=anchor aria-hidden=true href=#feed-forward-network-ffn>#</a></h3><p>FFN은 다음과 같은 구조를 가집니다:</p><ol><li><strong>Linear</strong>: 입력을 차원 $d \rightarrow d_{FFN}$으로 확장</li><li><strong>GELU Activation</strong></li><li><strong>Linear</strong>: 차원을 다시 $d_{FFN} \rightarrow d$로 축소</li><li><strong>Residual + LayerNorm</strong></li></ol><p>FFN에는 2개의 linear 연산이 포함됩니다.</p><p> 
 </p><h2 id=linear-연산-matmul-dimension-정리>Linear 연산: Matmul Dimension 정리<a hidden class=anchor aria-hidden=true href=#linear-연산-matmul-dimension-정리>#</a></h2><p>아래 표는 각 모듈의 linear 연산과 연산 차원을 요약한 것입니다. <code>Q·K^T</code>와 <code>attention·V</code>는 **시퀀스 길이에 대해 quadratic하고, 나머지는 linear합니다.</p><table><thead><tr><th>Module</th><th>Operation</th><th>Matmul dim</th></tr></thead><tbody><tr><td>MHA</td><td>${W_Q}$ projection</td><td>$d$ x $d$ x $l$</td></tr><tr><td></td><td>${W_K}$ projection</td><td>$d$ x $d$ x $l$</td></tr><tr><td></td><td>${W_V}$ projection</td><td>$d$ x $d$ x $l$</td></tr><tr><td></td><td><strong>query x key</strong></td><td>$l$ x $d/h$ x $l$</td></tr><tr><td></td><td><strong>atten. score x value</strong></td><td>$d/h$ x $l$ x $l$</td></tr><tr><td></td><td>${W_{out}}$ projection</td><td>$d$ x $d$ x $l$</td></tr><tr><td>FFN</td><td>${W_1}$ projection</td><td>${d_{FFN}}$ x $d$ x $l$</td></tr><tr><td></td><td>${W_2}$ projection</td><td>${d}$ x ${d_{FFN}}$ x ${l}$</td></tr></tbody></table><ul><li>$l$ : sequence length</li><li>$d$ : hidden dimension</li><li>$h$ : attention heads</li></ul><p> 
 </p><h2 id=non-linear-연산>Non-linear 연산<a hidden class=anchor aria-hidden=true href=#non-linear-연산>#</a></h2><p>Transformer에는 다음과 같은 비선형 연산이 포함됩니다: <strong>Softmax</strong>, <strong>LayerNorm</strong>, <strong>GELU</strong></p><p>이들은 연산량은 적지만 <strong>메모리 접근 패턴</strong>과 <strong>연산 방식</strong> 때문에 오버헤드가 발생하기 쉽습니다. 논문의 다음 그림은 각 non-linear 연산이 어느 축으로 작동하고 어떻게 동작하는지 잘 보여줍니다. 초록색 표시가 계산 방향을 나타냅니다.
 </p><p><img alt=non-linear loading=lazy src=../../post/2025-05-10-transformer-architecture/image04.png></p><p> </p><p><strong>(a) Softmax - 시퀀스 길이를 따라 정규화</strong>
Softmax는 주로 어텐션 점수를 정규화할 때 사용되며, 한 행(= 시퀀스 길이) 안에서 exponential 값을 모두 더한 후 각 항목을 나눕니다.</p><ul><li>계산 방향: Sequence Length 축</li><li>특징:<ul><li>전체 합을 먼저 계산해야 softmax를 적용할 수 있음</li><li>한 번 전체를 스캔해야 하므로 1-pass로는 부족</li></ul></li><li>성능 이슈:<ul><li>입력 전체를 순회하며 합을 구해야 하므로 메모리 접근 비용이 큼</li></ul></li></ul><p><strong>(b) Layer Normalization – 히든 차원으로 정규화</strong>
LayerNorm은 Transformer에서 거의 모든 블록 뒤에 붙는 핵심 연산입니다. 각 토큰의 히든 벡터를 기준으로 평균과 표준편차를 계산하고 정규화합니다.</p><ul><li>계산 방향: Hidden Dimension 축 (한 토큰 기준)</li><li>특징:<ul><li>평균(μ), 표준편차(σ) 계산 → 정규화 → scale & shift</li><li>따라서 최소 3번 이상 입력을 순회해야 함</li></ul></li><li>성능 이슈:<ul><li>반복된 접근이 필요해서 메모리 대역폭 병목의 주요 원인 중 하나</li></ul></li></ul><p><strong>(c) Batch Normalization – 채널 단위, 학습된 통계 사용</strong>
BatchNorm은 주로 CNN에서 사용되고, 훈련 중에 채널별 통계(μ, σ)를 학습하여 저장합니다. 추론 시에는 학습된 평균과 표준편차를 그대로 사용하므로 속도가 빠릅니다.</p><ul><li>계산 방향: Channel 축 (Across batch/height/width)</li><li>특징:<ul><li>μ, σ 모두 훈련 중 학습되고 고정됨</li><li>추론 시 1-pass로 충분 (실시간 계산 없음)</li></ul></li><li>성능 장점:<ul><li>추론 시 가장 빠름</li><li>연산량 대비 메모리 접근도 적음</li></ul></li></ul><h2 id=transformer-block---decoder>Transformer Block - Decoder<a hidden class=anchor aria-hidden=true href=#transformer-block---decoder>#</a></h2><p>Decoder는 Encoder와 유사하지만 다음과 같은 차이가 있습니다:</p><ul><li>입력은 한 토큰 ($d × 1$)만 처리</li><li>이전 토큰들과 concat하여 MHA 수행 (auto-regressive)</li><li>미래 토큰을 참조하지 않도록 <strong>Masked Attention</strong> 적용</li><li>Attention은 <strong>matrix-vector</strong> 연산으로 수행됨 [ $d/h$ x $1$, $l$ x $d/h$ ]</li></ul><p><img alt=decoder loading=lazy src=../../post/2025-05-10-transformer-architecture/image02.png></p><p> 
 </p><h2 id=encoder-vs-decoder-구조-비교>Encoder vs. Decoder: 구조 비교<a hidden class=anchor aria-hidden=true href=#encoder-vs-decoder-구조-비교>#</a></h2><p><img alt=encoder-decoder loading=lazy src=../../post/2025-05-10-transformer-architecture/image03.png></p><p> </p><table><thead><tr><th>항목</th><th>인코더 (Encoder)</th><th>디코더 (Decoder)</th></tr></thead><tbody><tr><td><strong>입력 처리 방식</strong></td><td>전체 입력 시퀀스를 <strong>병렬적으로</strong> 처리</td><td>이전에 생성된 토큰들을 사용하여 <strong>순차적으로</strong> 처리 (auto-regressive)</td></tr><tr><td><strong>기본 구성</strong></td><td><ul><li>Multi-Head Self Attention (MHA)</li><li>Feed-Forward Network (FFN)</li></ul></td><td><ul><li>Masked Multi-Head Self Attention</li><li>Cross Attention (인코더 출력과 함께)</li><li>Feed-Forward Network (FFN)</li></ul></td></tr><tr><td><strong>Self-Attention</strong></td><td>입력 전체에 대해 자유롭게 어텐션 가능</td><td><strong>미래 정보 차단</strong>을 위한 <strong>마스킹(masked)</strong> 어텐션 사용</td></tr><tr><td><strong>Cross-Attention 존재 여부</strong></td><td>없음 (입력 자체만 사용)</td><td>있음 (인코더의 출력과 디코더의 입력 간 어텐션)</td></tr><tr><td><strong>입력/출력 예시</strong></td><td>&ldquo;I am a student&rdquo; → 인코딩된 벡터</td><td>&ldquo;나는 학생입니다&rdquo; 생성 (한 토큰씩)</td></tr><tr><td><strong>용도</strong></td><td>자연어 이해 (e.g., 문장 분류, 요약)</td><td>자연어 생성 (e.g., 기계 번역, 텍스트 생성)</td></tr><tr><td><strong>병렬성</strong></td><td>높음 (모든 토큰을 동시에 처리 가능)</td><td>낮음 (각 토큰이 이전 토큰에 의존함)</td></tr><tr><td><strong>캐시 활용</strong></td><td>일반적으로 없음</td><td>Key/Value 캐싱으로 이전 연산 결과 재사용 → <strong>성능 향상</strong></td></tr></tbody></table><hr><p>Transformer의 대략적인 구조와 각 블락을 이루는 연산 단위를 살펴보았습니다. 다음 글에서는 논문의 2.2장 Model Analysis를 기반으로 BERT 모델과 GPT 모델의 병목 지점을 알아보겠습니다.</p><p> 
 </p></div><footer class=post-footer><ul class=post-tags><li><a href=http://huijeong-kim.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=http://huijeong-kim.github.io/tags/inference/>Inference</a></li></ul><nav class=paginav><a class=prev href=http://huijeong-kim.github.io/post/2025-05-26-transformer-model-workload-analysis/><span class=title>« Prev</span><br><span>Transformer Model Workload Analysis</span>
</a><a class=next href=http://huijeong-kim.github.io/post/2024-02-02-cargo-build-run-if-changed/><span class=title>Next »</span><br><span>빌드 스크립트 build.rs</span></a></nav></footer><script src=https://utteranc.es/client.js repo=huijeong-kim/huijeong-kim.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=http://huijeong-kim.github.io/>Dev. note</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>