<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Transformer Model Workload Analysis | Dev. note</title>
<meta name=keywords content="machine learning,inference"><meta name=description content="‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 2편
논문의 2.2장에서는 모델 워크로드를 분석합니다. Idle한 상황을 가정하고, 각 트랜스포머 모델의 이론적 최대 성능(upper bound)을 분석합니다. 그 과정에서 각 모델의 특성을 이해해 볼 수 있습니다.
논문 링크: https://arxiv.org/abs/2302.14017

 
Models
논문은 BERT-Base, BERT-Large, GPT-2 모델로 워크로드를 분석했습니다. 모두 트랜스포머 기반입니다. 주요 특징과 parameter configuration은 아래와 같습니다.

  
      
          모델
          구조
          방향성
          목적
          주요 용도
      
  
  
      
          12-layer BERT-Base
          인코더-only
          양방향
          마스킹된 단어 예측 (MLM)
          문장 이해 (분류, 질의응답 등)
      
      
          24-layer BERT-Large
          인코더-only
          양방향
          BERT-Base 확장
          고성능 문장 이해
      
      
          12-layer GPT-2
          디코더-only
          단방향 (왼→오)
          다음 단어 예측 (causal LM)
          텍스트 생성 (요약, 번역, 대화 등)
      
  

 "><meta name=author content="Huijeong Kim"><link rel=canonical href=http://huijeong-kim.github.io/post/2025-05-26-transformer-model-workload-analysis/><link crossorigin=anonymous href=../../assets/css/stylesheet.f49d66caae9ea0fd43f21f29e71a8d3e284517ed770f2aa86fa012953ad3c9ef.css integrity="sha256-9J1myq6eoP1D8h8p5xqNPihFF+13Dyqob6ASlTrTye8=" rel="preload stylesheet" as=style><link rel=icon href=http://huijeong-kim.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://huijeong-kim.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://huijeong-kim.github.io/favicon-32x32.png><link rel=apple-touch-icon href=http://huijeong-kim.github.io/apple-touch-icon.png><link rel=mask-icon href=http://huijeong-kim.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://huijeong-kim.github.io/post/2025-05-26-transformer-model-workload-analysis/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js async></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-VHBQNPGKKB"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-VHBQNPGKKB")}</script><meta property="og:url" content="http://huijeong-kim.github.io/post/2025-05-26-transformer-model-workload-analysis/"><meta property="og:site_name" content="Dev. note"><meta property="og:title" content="Transformer Model Workload Analysis"><meta property="og:description" content="‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 2편
논문의 2.2장에서는 모델 워크로드를 분석합니다. Idle한 상황을 가정하고, 각 트랜스포머 모델의 이론적 최대 성능(upper bound)을 분석합니다. 그 과정에서 각 모델의 특성을 이해해 볼 수 있습니다.
논문 링크: https://arxiv.org/abs/2302.14017
Models 논문은 BERT-Base, BERT-Large, GPT-2 모델로 워크로드를 분석했습니다. 모두 트랜스포머 기반입니다. 주요 특징과 parameter configuration은 아래와 같습니다.
모델 구조 방향성 목적 주요 용도 12-layer BERT-Base 인코더-only 양방향 마스킹된 단어 예측 (MLM) 문장 이해 (분류, 질의응답 등) 24-layer BERT-Large 인코더-only 양방향 BERT-Base 확장 고성능 문장 이해 12-layer GPT-2 디코더-only 단방향 (왼→오) 다음 단어 예측 (causal LM) 텍스트 생성 (요약, 번역, 대화 등) "><meta property="og:locale" content="kr"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2025-05-26T07:31:04+09:00"><meta property="article:modified_time" content="2025-05-26T07:31:04+09:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Inference"><meta name=twitter:card content="summary"><meta name=twitter:title content="Transformer Model Workload Analysis"><meta name=twitter:description content="‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 2편
논문의 2.2장에서는 모델 워크로드를 분석합니다. Idle한 상황을 가정하고, 각 트랜스포머 모델의 이론적 최대 성능(upper bound)을 분석합니다. 그 과정에서 각 모델의 특성을 이해해 볼 수 있습니다.
논문 링크: https://arxiv.org/abs/2302.14017

 
Models
논문은 BERT-Base, BERT-Large, GPT-2 모델로 워크로드를 분석했습니다. 모두 트랜스포머 기반입니다. 주요 특징과 parameter configuration은 아래와 같습니다.

  
      
          모델
          구조
          방향성
          목적
          주요 용도
      
  
  
      
          12-layer BERT-Base
          인코더-only
          양방향
          마스킹된 단어 예측 (MLM)
          문장 이해 (분류, 질의응답 등)
      
      
          24-layer BERT-Large
          인코더-only
          양방향
          BERT-Base 확장
          고성능 문장 이해
      
      
          12-layer GPT-2
          디코더-only
          단방향 (왼→오)
          다음 단어 예측 (causal LM)
          텍스트 생성 (요약, 번역, 대화 등)
      
  

 "><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://huijeong-kim.github.io/post/"},{"@type":"ListItem","position":2,"name":"Transformer Model Workload Analysis","item":"http://huijeong-kim.github.io/post/2025-05-26-transformer-model-workload-analysis/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Transformer Model Workload Analysis","name":"Transformer Model Workload Analysis","description":"‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 2편\n논문의 2.2장에서는 모델 워크로드를 분석합니다. Idle한 상황을 가정하고, 각 트랜스포머 모델의 이론적 최대 성능(upper bound)을 분석합니다. 그 과정에서 각 모델의 특성을 이해해 볼 수 있습니다.\n논문 링크: https://arxiv.org/abs/2302.14017\nModels 논문은 BERT-Base, BERT-Large, GPT-2 모델로 워크로드를 분석했습니다. 모두 트랜스포머 기반입니다. 주요 특징과 parameter configuration은 아래와 같습니다.\n모델 구조 방향성 목적 주요 용도 12-layer BERT-Base 인코더-only 양방향 마스킹된 단어 예측 (MLM) 문장 이해 (분류, 질의응답 등) 24-layer BERT-Large 인코더-only 양방향 BERT-Base 확장 고성능 문장 이해 12-layer GPT-2 디코더-only 단방향 (왼→오) 다음 단어 예측 (causal LM) 텍스트 생성 (요약, 번역, 대화 등) ","keywords":["machine learning","inference"],"articleBody":"‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 2편\n논문의 2.2장에서는 모델 워크로드를 분석합니다. Idle한 상황을 가정하고, 각 트랜스포머 모델의 이론적 최대 성능(upper bound)을 분석합니다. 그 과정에서 각 모델의 특성을 이해해 볼 수 있습니다.\n논문 링크: https://arxiv.org/abs/2302.14017\nModels 논문은 BERT-Base, BERT-Large, GPT-2 모델로 워크로드를 분석했습니다. 모두 트랜스포머 기반입니다. 주요 특징과 parameter configuration은 아래와 같습니다.\n모델 구조 방향성 목적 주요 용도 12-layer BERT-Base 인코더-only 양방향 마스킹된 단어 예측 (MLM) 문장 이해 (분류, 질의응답 등) 24-layer BERT-Large 인코더-only 양방향 BERT-Base 확장 고성능 문장 이해 12-layer GPT-2 디코더-only 단방향 (왼→오) 다음 단어 예측 (causal LM) 텍스트 생성 (요약, 번역, 대화 등) Symbol Parameter BERT-Base BERT-Large GPT-2 $N$ # Layers 12 24 12 $d$ Model dimension 768 1024 768 $h$ # Attention Heads 12 16 12 $d_{FFN}$ FFN dimension 3072 4096 3072 Assumptions BERT 모델의 최대 입력 Sequence number $l$ 인 512를 무시 8-bit(1Byte) precision for all operations 무한한 메모리 사용 가정 - upper bound performance Arithmetic Intensity 모델의 실행 효율은 다음 두 지표에 의해 표현될 수 있습니다.\nFLOPs: 부동소수점 연산 수 MOPs: 메모리 접근 수 이 지표의 비율인 Arithmetic Intensity를 사용하여 모델을 비교합니다. Arithmetic Intensity는 메모리 접근 1Byte 당 수행될 수 있는 floating-point operation 수 입니다.\n$$ \\text{Arithmetic Intensity} = \\frac{\\text{FLOPs}}{\\text{MOPs}} $$\nArithmetic intensity를 통해 모델의 성능이 compute-bound인지 memory-bound인지 알 수 있습니다. 동일 FLOPs를 가지는 모델이라면 arithmetic intensity가 큰 모델이 메모리 병목으로 인한 성능 저하가 적을 것이므로 비슷하거나 더 높은 성능을 가질 수 있습니다.\nE2E 워크로드 특성 FLOPs and MOPs FLOPs 와 MOPs는 sequence length 에 super-linear(초선형적)입니다 Sequence length 증가에 따라 FLOPs와 MOPs가 급격히 증가하고, 이는 act-to-act matmul(query x key, attention score x value) 연산이 sequence length에 quadratic 하기 때문입니다 (참고: 지난 포스트) Arithmetic Intensity BERT-* 모델은 sequence length 512까지는 arithmetic intensity가 증가하나, 그 이후부터 줄어듭니다 Sequence length 가 커지면 더 큰 dimension의 matmul이 발생하고, parameter load 당 더 많은 연산을 수행하기 때문에 자연스레 arithmetic intensity가 증가합니다. 하지만 sequence length가 512보다 커지면 FFN 모듈보다 MHA 모듈이 dominate하는 양상을 보입니다 - MHA 모듈의 act-to-act matmul과 Softmax 연산이 두드러지게 됩니다. 이와 관련된 내용은 다음 섹션에서 자세히 다룹니다. BERT 대비 GPT 모델은 확연히 낮은 arithmetic intensity를 보여줍니다 Decoder 모델은 matrix-matrix가 아닌 matrix-vector 연산으로만 이루어져 있기 때문입니다 - 이는 데이터 재활용률을 낮춥니다. 성능은 memory bandwidth-bound가 됩니다. Per-layer 특성 다음은 Intel Gold 6242 CPU에서의 레이어 별 프로파일링 결과입니다.\nBERT-base 모델에서는 sequence length가 작을 때는 FFN이, 클 때는 MHA 연산이 dominate 합니다 GPT 모델에서는 MHA 연산이 큰 비중을 이루고, sequence length가 커지면 MHA 연산 비중이 높아집니다 (BERT-base 만큼의 비중은 아니지만요) 각 모델의 normalized latency를 보여주는 그림 9는 BERT 모델과 GPT 모델의 차이를 보여줍니다. 더 높은 arithmetic intensity를 가진 모델(BERT)은 더 빠르게 수행되고 있음을, decoder inference는 memory-bound problem임을 보여줍니다.\nPer-layer 실행 특성을 정리하면 다음과 같습니다.\n모듈 연산 비중 메모리 접근 비중 병목 원인 Attention (QKV projection + attention score) 높음 중간 QKᵀ와 softmax의 메모리 병목 FFN (2-layer MLP) 가장 많은 FLOPs 비교적 낮은 MOPs MatMul 중심, 효율적 LayerNorm + residuals FLOPs 적음 MOPs 많음 메모리 접근 병목, 낮은 AI 실제로 FFN이 전체 연산량의 대부분을 차지하지만, 실행 속도를 결정하는 건 softmax, norm 등 low-intensity 연산의 병목 효과입니다. “실제 병목은 FLOPs가 많은 곳이 아니라, MOPs가 많은 곳에서 발생한다” 는 것을 알 수 있습니다.\nMatMul (GEMM) 높은 FLOPs, 낮은 MOPs → 높은 Arithmetic Intensity 하드웨어에서 효율적으로 실행 가능 Softmax, LayerNorm, GELU 낮은 FLOPs, 높은 MOPs → 낮은 Arithmetic Intensity 메모리 접근 병목 발생 이번 글에서는 세 가지 트랜스포머 모델의 워크로드 특성과 병목 지점 분석 내용을 리뷰해 봤습니다. FLOPs가 많은 연산이 아니라, 메모리 접근이 많은 연산이 실제 성능을 좌우합니다. 특히 decoder-only 구조인 GPT-2는 낮은 arithmetic intensity로 인해 memory-bound 특성이 강하게 나타났으며, 이는 인퍼런스 최적화에서 중요한 고려 요소가 됩니다. 다음 글에서는 일반적인 DNN accelerator의 특성에 대해 알아볼 예정입니다.\n","wordCount":"595","inLanguage":"en","datePublished":"2025-05-26T07:31:04+09:00","dateModified":"2025-05-26T07:31:04+09:00","author":{"@type":"Person","name":"Huijeong Kim"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://huijeong-kim.github.io/post/2025-05-26-transformer-model-workload-analysis/"},"publisher":{"@type":"Organization","name":"Dev. note","logo":{"@type":"ImageObject","url":"http://huijeong-kim.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://huijeong-kim.github.io/ accesskey=h title="Dev. note (Alt + H)">Dev. note</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://huijeong-kim.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=http://huijeong-kim.github.io/post/ title=Posts><span>Posts</span></a></li><li><a href=http://huijeong-kim.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://huijeong-kim.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://huijeong-kim.github.io/>Home</a>&nbsp;»&nbsp;<a href=http://huijeong-kim.github.io/post/>Posts</a></div><h1 class="post-title entry-hint-parent">Transformer Model Workload Analysis</h1><div class=post-meta><span title='2025-05-26 07:31:04 +0900 +0900'>2025-05-26</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;595 words&nbsp;·&nbsp;Huijeong Kim</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#models aria-label=Models>Models</a><ul><ul><li><a href=#assumptions aria-label=Assumptions>Assumptions</a></li></ul></ul></li><li><a href=#arithmetic-intensity aria-label="Arithmetic Intensity">Arithmetic Intensity</a></li><li><a href=#e2e-%ec%9b%8c%ed%81%ac%eb%a1%9c%eb%93%9c-%ed%8a%b9%ec%84%b1 aria-label="E2E 워크로드 특성">E2E 워크로드 특성</a><ul><ul><li><a href=#flops-and-mops aria-label="FLOPs and MOPs">FLOPs and MOPs</a></li><li><a href=#arithmetic-intensity-1 aria-label="Arithmetic Intensity">Arithmetic Intensity</a></li></ul></ul></li><li><a href=#per-layer-%ed%8a%b9%ec%84%b1 aria-label="Per-layer 특성">Per-layer 특성</a></li></ul></div></details></div><div class=post-content><p><em>‘Full Stack Optimization of Transformer Inference: a Survey’ 리뷰 시리즈 2편</em></p><p>논문의 2.2장에서는 모델 워크로드를 분석합니다. Idle한 상황을 가정하고, 각 트랜스포머 모델의 이론적 최대 성능(upper bound)을 분석합니다. 그 과정에서 각 모델의 특성을 이해해 볼 수 있습니다.</p><p>논문 링크: <a href=https://arxiv.org/abs/2302.14017>https://arxiv.org/abs/2302.14017</a></p><hr><p> </p><h2 id=models>Models<a hidden class=anchor aria-hidden=true href=#models>#</a></h2><p>논문은 BERT-Base, BERT-Large, GPT-2 모델로 워크로드를 분석했습니다. 모두 트랜스포머 기반입니다. 주요 특징과 parameter configuration은 아래와 같습니다.</p><table><thead><tr><th>모델</th><th>구조</th><th>방향성</th><th>목적</th><th>주요 용도</th></tr></thead><tbody><tr><td>12-layer BERT-Base</td><td>인코더-only</td><td>양방향</td><td>마스킹된 단어 예측 (MLM)</td><td>문장 이해 (분류, 질의응답 등)</td></tr><tr><td>24-layer BERT-Large</td><td>인코더-only</td><td>양방향</td><td>BERT-Base 확장</td><td>고성능 문장 이해</td></tr><tr><td>12-layer GPT-2</td><td>디코더-only</td><td>단방향 (왼→오)</td><td>다음 단어 예측 (causal LM)</td><td>텍스트 생성 (요약, 번역, 대화 등)</td></tr></tbody></table><p> </p><table><thead><tr><th>Symbol</th><th>Parameter</th><th>BERT-Base</th><th>BERT-Large</th><th>GPT-2</th></tr></thead><tbody><tr><td>$N$</td><td># Layers</td><td>12</td><td>24</td><td>12</td></tr><tr><td>$d$</td><td>Model dimension</td><td>768</td><td>1024</td><td>768</td></tr><tr><td>$h$</td><td># Attention Heads</td><td>12</td><td>16</td><td>12</td></tr><tr><td>$d_{FFN}$</td><td>FFN dimension</td><td>3072</td><td>4096</td><td>3072</td></tr></tbody></table><p> </p><h4 id=assumptions>Assumptions<a hidden class=anchor aria-hidden=true href=#assumptions>#</a></h4><ul><li>BERT 모델의 최대 입력 Sequence number $l$ 인 512를 무시</li><li>8-bit(1Byte) precision for all operations</li><li>무한한 메모리 사용 가정 - upper bound performance</li></ul><p> </p><h2 id=arithmetic-intensity>Arithmetic Intensity<a hidden class=anchor aria-hidden=true href=#arithmetic-intensity>#</a></h2><p>모델의 실행 효율은 다음 두 지표에 의해 표현될 수 있습니다.</p><ul><li><strong>FLOPs</strong>: 부동소수점 연산 수</li><li><strong>MOPs</strong>: 메모리 접근 수</li></ul><p>이 지표의 비율인 Arithmetic Intensity를 사용하여 모델을 비교합니다. Arithmetic Intensity는 메모리 접근 1Byte 당 수행될 수 있는 floating-point operation 수 입니다.</p><p>$$
\text{Arithmetic Intensity} = \frac{\text{FLOPs}}{\text{MOPs}}
$$</p><p>Arithmetic intensity를 통해 모델의 성능이 compute-bound인지 memory-bound인지 알 수 있습니다. 동일 FLOPs를 가지는 모델이라면 arithmetic intensity가 큰 모델이 메모리 병목으로 인한 성능 저하가 적을 것이므로 비슷하거나 더 높은 성능을 가질 수 있습니다.</p><p> </p><h2 id=e2e-워크로드-특성>E2E 워크로드 특성<a hidden class=anchor aria-hidden=true href=#e2e-워크로드-특성>#</a></h2><h4 id=flops-and-mops>FLOPs and MOPs<a hidden class=anchor aria-hidden=true href=#flops-and-mops>#</a></h4><p><img alt="flops and mops" loading=lazy src=../../post/2025-05-26-transformer-model-workload-analysis/image01.png></p><ul><li>FLOPs 와 MOPs는 sequence length 에 super-linear(초선형적)입니다</li><li>Sequence length 증가에 따라 FLOPs와 MOPs가 급격히 증가하고, 이는 act-to-act matmul(query x key, attention score x value) 연산이 sequence length에 quadratic 하기 때문입니다 (참고: <a href=https://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/#linear-%EC%97%B0%EC%82%B0-matmul-dimension-%EC%A0%95%EB%A6%AC>지난 포스트</a>)</li></ul><p> </p><h4 id=arithmetic-intensity-1>Arithmetic Intensity<a hidden class=anchor aria-hidden=true href=#arithmetic-intensity-1>#</a></h4><p><img alt="arithmetic intensity" loading=lazy src=../../post/2025-05-26-transformer-model-workload-analysis/image02.png></p><ul><li>BERT-* 모델은 sequence length 512까지는 arithmetic intensity가 증가하나, 그 이후부터 줄어듭니다<ul><li>Sequence length 가 커지면 더 큰 dimension의 matmul이 발생하고, parameter load 당 더 많은 연산을 수행하기 때문에 자연스레 arithmetic intensity가 증가합니다.</li><li>하지만 sequence length가 512보다 커지면 FFN 모듈보다 MHA 모듈이 dominate하는 양상을 보입니다 - MHA 모듈의 act-to-act matmul과 Softmax 연산이 두드러지게 됩니다. 이와 관련된 내용은 다음 섹션에서 자세히 다룹니다.</li></ul></li><li>BERT 대비 GPT 모델은 확연히 낮은 arithmetic intensity를 보여줍니다<ul><li>Decoder 모델은 matrix-matrix가 아닌 matrix-vector 연산으로만 이루어져 있기 때문입니다 - 이는 데이터 재활용률을 낮춥니다.</li><li>성능은 memory bandwidth-bound가 됩니다.</li></ul></li></ul><p> 
 </p><h2 id=per-layer-특성>Per-layer 특성<a hidden class=anchor aria-hidden=true href=#per-layer-특성>#</a></h2><p>다음은 Intel Gold 6242 CPU에서의 레이어 별 프로파일링 결과입니다.</p><p><img alt="latency profiling" loading=lazy src=../../post/2025-05-26-transformer-model-workload-analysis/image03.png></p><ul><li>BERT-base 모델에서는 sequence length가 작을 때는 FFN이, 클 때는 MHA 연산이 dominate 합니다</li><li>GPT 모델에서는 MHA 연산이 큰 비중을 이루고, sequence length가 커지면 MHA 연산 비중이 높아집니다 (BERT-base 만큼의 비중은 아니지만요)</li></ul><p> </p><p><img alt="normalized latency" loading=lazy src=../../post/2025-05-26-transformer-model-workload-analysis/image04.png></p><p>각 모델의 normalized latency를 보여주는 그림 9는 BERT 모델과 GPT 모델의 차이를 보여줍니다. 더 높은 arithmetic intensity를 가진 모델(BERT)은 더 빠르게 수행되고 있음을, decoder inference는 memory-bound problem임을 보여줍니다.</p><p> </p><p>Per-layer 실행 특성을 정리하면 다음과 같습니다.</p><table><thead><tr><th>모듈</th><th>연산 비중</th><th>메모리 접근 비중</th><th>병목 원인</th></tr></thead><tbody><tr><td><strong>Attention</strong> (QKV projection + attention score)</td><td>높음</td><td>중간</td><td>QKᵀ와 softmax의 메모리 병목</td></tr><tr><td><strong>FFN</strong> (2-layer MLP)</td><td>가장 많은 FLOPs</td><td>비교적 낮은 MOPs</td><td>MatMul 중심, 효율적</td></tr><tr><td><strong>LayerNorm + residuals</strong></td><td>FLOPs 적음</td><td>MOPs 많음</td><td>메모리 접근 병목, 낮은 AI</td></tr></tbody></table><p>실제로 FFN이 전체 연산량의 대부분을 차지하지만, 실행 속도를 결정하는 건 softmax, norm 등 low-intensity 연산의 병목 효과입니다. <strong>&ldquo;실제 병목은 FLOPs가 많은 곳이 아니라, MOPs가 많은 곳에서 발생한다&rdquo;</strong>
는 것을 알 수 있습니다.</p><ul><li><strong>MatMul (GEMM)</strong><ul><li>높은 FLOPs, 낮은 MOPs → 높은 Arithmetic Intensity</li><li>하드웨어에서 효율적으로 실행 가능</li></ul></li><li><strong>Softmax, LayerNorm, GELU</strong><ul><li>낮은 FLOPs, 높은 MOPs → 낮은 Arithmetic Intensity</li><li>메모리 접근 병목 발생</li></ul></li></ul><hr><p>이번 글에서는 세 가지 트랜스포머 모델의 워크로드 특성과 병목 지점 분석 내용을 리뷰해 봤습니다. FLOPs가 많은 연산이 아니라, 메모리 접근이 많은 연산이 실제 성능을 좌우합니다. 특히 decoder-only 구조인 GPT-2는 낮은 arithmetic intensity로 인해 memory-bound 특성이 강하게 나타났으며, 이는 인퍼런스 최적화에서 중요한 고려 요소가 됩니다. 다음 글에서는 일반적인 DNN accelerator의 특성에 대해 알아볼 예정입니다.</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://huijeong-kim.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=http://huijeong-kim.github.io/tags/inference/>Inference</a></li></ul><nav class=paginav><a class=next href=http://huijeong-kim.github.io/post/2025-05-10-transformer-architecture/><span class=title>Next »</span><br><span>Transformer Architecture - 구조와 연산 소개</span></a></nav></footer><script src=https://utteranc.es/client.js repo=huijeong-kim/huijeong-kim.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=http://huijeong-kim.github.io/>Dev. note</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>